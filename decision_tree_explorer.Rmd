---
title: "Decision Tree Explorer"
subtitle: "Microsoft Team Data Science Process"
version: "Version 0.9f"
output: html_document
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, message=FALSE)
```

```{r config}
library(jsonlite)
library(dplyr)

HIGHLIGHT_COLOR <- "red"

DATA_DIR <- "models"

DATA_MODELS <- gsub("\\.Rds$", "", list.files(DATA_DIR, pattern="*.Rds"))

```


```{r load_tree}

loadTree <- function(data_file){
  tree <- readRDS(file.path(DATA_DIR, paste0(data_file, ".Rds")))
  if (!("rpart" %in% class(tree))) tree <- as.rpart(tree) # handle rxDTree objects without having to know what they are
  tree
}

best_cp <- function(TREE){
  # rxDTreeBestCp(TREE, nstd=0)
  best_cp_row <- which(TREE$cptable[,'xerror'] == min(TREE$cptable[,'xerror']))
  # If there are ties, we'll take the last one
  TREE$cptable[max(best_cp_row),"CP"]
}

tree_performance_stats <- function(myTree){
  
  num_obs <- myTree$frame[1,"n"]
  num_obs2 <- myTree$frame[1,"n_test"]
  
  leaf_values <- myTree$frame[myTree$frame$var=="<leaf>", c("n", "yval","n_test", "yval_test", "rule")]
  leaf_values$percent <- 100 * leaf_values$n / num_obs
  leaf_values$percent_test <- 100 * leaf_values$n_test / num_obs2
  
  leaf_values <- leaf_values[order(leaf_values$yval, decreasing=TRUE),] # order by model scores
  
  leaf_values <- transform(leaf_values, 
                           TP=n * yval, 
                           FP=n * (1 - yval),
                           TP_test = n_test * yval_test,
                           FP_test = n_test * (1 - yval_test))
 
  leaf_values <- transform(leaf_values,
                           pop_fraction = cumsum(n)/sum(n),
                           TPR=cumsum(TP)/sum(TP), 
                           FPR=cumsum(FP)/sum(FP),
                           pop_fraction_test = cumsum(n_test)/sum(n_test),
                           TPR_test=cumsum(TP_test)/sum(TP_test), 
                           FPR_test=cumsum(FP_test)/sum(FP_test))
  leaf_values
}

### 
get_node_id_by_liftclick <- function(click, leaf_val_df, dataset){
  pop_column <- if (dataset=="TEST") "pop_fraction_test" else "pop_fraction"
  selected_node <- row.names(leaf_val_df)[min(which((100*leaf_val_df[[pop_column]]) > click$x))]
  yval_column <- if (dataset=="TEST") "yval_test" else "yval"
  # if (100*leaf_val_df[selected_node, yval_column] < click$y) selected_node <- character(0) ## triggers redraw (???)
  if (is.na(selected_node)) selected_node <- character(0)
  return(selected_node)
}

get_node_id_by_treeclick <- function(plotted_tree, click){
  node_idx <- with(plotted_tree$boxes,(
       (x1 < click$x) & 
       (x2 > click$x) &
       (y1 < click$y) & 
       (y2 > click$y) 
  ))
  nodes <- row.names(plotted_tree$obj$frame)
  nodes[node_idx]
}

plot_tree <- function(pTree, dataset, selectedNode=character(0)){
  library(rpart.plot)
  # Copy test results over training results. Be aware that this only works for methods that
  # take all their numbers from the tree frame. Note that rpart.plot with extra=100 does NOT
  # compute percents correctly when we lie to it like this, but the counts seem to work right.
  if (dataset=="TEST"){ 
    pTree$frame$n <- pTree$frame$n_test
    pTree$frame$yval <- pTree$frame$yval_test
  }
  rpart.plot(pTree, 
             extra=1, #  display number, not percent in each node; percent is incorrect for test set.
             nn=TRUE, xflip=TRUE, # border.col
             shadow.col= if (length(selectedNode) == 0)
                           'black'
                         else
                           ifelse(row.names(pTree$frame)==selectedNode, 
                                HIGHLIGHT_COLOR, 'black')) 
}

get_node_info <- function(nodeID, plotted_tree, dataset){
  node_info <- list(nodeID=nodeID)
  leaf_val_df <- tree_performance_stats(plotted_tree)
  node_row <- plotted_tree$frame[nodeID,]
  node_info$rule <- node_row$rule
  if (dataset == "TEST"){
    node_info$num_cases <- node_row$n_test
    node_info$outcome <- node_row$yval_test
    node_info$total_cases <- sum(leaf_val_df$n_test)
    node_info$total_positive <- sum(leaf_val_df$TP_test)
  } else {
    node_info$num_cases <- node_row$n
    node_info$outcome <- node_row$yval
    node_info$total_cases <- sum(leaf_val_df$n)
    node_info$total_positive <- sum(leaf_val_df$TP)
  }
  formula_parts <- as.character(TREE$terms)
  node_info$model_formula <- paste0(formula_parts[2], formula_parts[1], formula_parts[3])
  node_info
}

show_node_info <- function(nodeID, plotted_tree, dataset){
  node_info <- get_node_info(nodeID, plotted_tree, dataset)
  with(node_info, {
    info <- paste0(ifelse(is.null(TREE$model_notes), '', paste0("notes: ", TREE$model_notes, "\n")),
           "model formula: ", model_formula)
    if (length(nodeID) > 0){
       selected_node_info <- paste0("\nnodeID: ", nodeID,
           "\nnumber of cases: ", num_cases, 
           " (", round(100*num_cases/total_cases, 2), "% of total)",
           "\npercent positive: ", round(100 * outcome, 2),
           " (", outcome * num_cases, " positive cases)",
           "\noverall percent positive: ", round(100 * total_positive/total_cases, 2),
           " (relative lift: ", round(100*outcome/(total_positive/total_cases), 2),"%)",
           "\nrule=", rule) 
       info <- paste(info, selected_node_info)
    }
    info
  })
}

plot_gain <- function(fullTree, prunedTree, dataset="TRAIN", selectedNode="None"){
  leaf_values_full <- tree_performance_stats(fullTree)
  leaf_values_pruned <- tree_performance_stats(prunedTree)

  # Cumulative gain curve
  if (dataset=="TEST"){
    test_vars <- grep("_test$", names(leaf_values_pruned), value=TRUE)
    training_vars <- gsub("_test$", "", test_vars)
    names(training_vars) <- test_vars
    
    for (var_name in names(training_vars)){
      leaf_values_full[[training_vars[var_name]]] <- leaf_values_full[[var_name]]
      leaf_values_pruned[[training_vars[var_name]]] <- leaf_values_pruned[[var_name]]
    }
  }
  
  with(leaf_values_full,{ 
       # Performance of full model
       plot(c(0, 100*pop_fraction), c(0, 100*TPR), type='l', lwd=2, col="orange", 
            main="Cumulative Gain Curve",
            xlab="Percent of population", ylab="Percent of all positive cases")
       # Ideal performance
       lines(c(0, 100*sum(TP)/sum(n), 100), c(0, 100, 100), lwd=2, lty=3, col="gray")
  })
  highlighted_point <- c(FALSE, row.names(leaf_values_pruned) == selectedNode)
  with(leaf_values_pruned,{
       lines(c(0, 100*pop_fraction), c(0, 100*TPR), 
             col=ifelse(highlighted_point, HIGHLIGHT_COLOR, "black"), 
             pch=ifelse(highlighted_point, 19, 1),
             lwd=2, lty=2, type='b')
       if(any(highlighted_point)){
          x0 <- 100*c(0,pop_fraction)[which(highlighted_point) - 1]
          y0 <- 100*c(0,TPR)[which(highlighted_point) - 1]
          lines(c(0, x0, x0, x0), c(y0, y0, 0, 0), col="darkgreen", lty=3)
          x <- 100*c(0,pop_fraction)[highlighted_point]
          y <- 100*c(0,TPR)[highlighted_point]
          lines(c(0, x, x, x), c(y, y, 0, 0), col=HIGHLIGHT_COLOR, lty=3)
        }
  })
  abline(0, 1, col="blue")
  
  # add gain curve for reference model if available
  if (!is.null(fullTree$reference_model_gain)){
    if(dataset=="TEST"){
      reference_model_gain <- fullTree$reference_model_gain$test
    } else {
      reference_model_gain <- fullTree$reference_model_gain$train
    }
    with(reference_model_gain, lines(100*x, 100*y, lty=3, lwd=2, col="darkgreen"))
  }

}


plot_lift <- function(prunedTree, dataset="TRAIN", selectedNode=character(0)){
  lvp <- tree_performance_stats(prunedTree) # leaf_values, pruned
  bar_colors <- ifelse( row.names(lvp) == selectedNode, HIGHLIGHT_COLOR, "darkgray")
  if (length(bar_colors) == 0) bar_colors <- "darkgray"

  # Lift plot
  if (dataset=="TEST"){
    num_obs <-  sum(lvp$n_test)
    
    overall_value <- 100*sum(lvp$n_test * lvp$yval_test)/num_obs
    height_vec <- 100*lvp$yval_test
    width_vec <- lvp$percent_test
  } else {
    num_obs <- sum(lvp$n)
    
    overall_value <- 100*sum(lvp$n * lvp$yval)/num_obs
    height_vec <- 100*lvp$yval
    width_vec <- lvp$percent
  }
  
  barplot(height=height_vec, width=width_vec, space=0, col=bar_colors, xlim=c(0,100),
          main="Lift Plot", ylab="Percent cases positive", xlab="Percent of population")
  abline(h=overall_value, col="blue")

}

```

```{r shiny_app}
get_complexity_map <- function(rpart_tree){
  function(relative_complexity){
    opt_cp <- best_cp(rpart_tree)  
    min_cp <- min(rpart_tree$frame$complexity[rpart_tree$frame$complexity!=0])
    max_cp <- max(rpart_tree$frame$complexity)
    
    if ((opt_cp <= min_cp) | (opt_cp >= max_cp)){
      opt_cp <- (min_cp + max_cp)/2
      ### warn("tree may be undertrained")
    }
    # cp_levels - unique(TREE$frame$complexity)
    
    cp_map <- data.frame(
      log_cp = c(seq(log(max_cp), log(opt_cp), length=100), 
             log(opt_cp),
             seq(log(opt_cp), log(min_cp), length=100)),
      std_cp = seq(-100, 100, by=1)
    )
    exp(cp_map[cp_map$std_cp==relative_complexity, "log_cp"])
  }
}

library(shiny)

TREE <- loadTree(DATA_MODELS[1])

ui <- fluidPage(
  fluidRow(
    column(12,
      inputPanel(
        selectInput("data_model", label = "Model file:",
              choices = DATA_MODELS),
        sliderInput("std_complexity", label = "Complexity adjustment:",
                    min = -100, max = 100, 
                    value = 0, 
                    step = 1),
        radioButtons("dataset", "data set to display:", 
             choices = list(test="TEST", training="TRAIN"), 
             selected = "TEST", inline = TRUE, width = NULL)
      )
    )
  ),
  fluidRow(
    column(8,
      fluidRow(plotOutput("treeplot", click = "treeplot_click")),
      fluidRow(tableOutput("rule_table")),
      fluidRow(verbatimTextOutput("node_info"))
    ),
    column(4,
      fluidRow(plotOutput("gainplot")),
      fluidRow(plotOutput("liftplot", click = "liftplot_click"))
    )
  )
)

server <- function(input, output) {

  CP <- NA
  SELECTED_NODE <- character(0)
  PLOTTED_TREE <- NULL
  PRUNED_TREE <- NULL
  STD_CP_FUN <- get_complexity_map(TREE)

  get_clicks <- reactive({
    # dataFile <- file.path(DATA_DIR, paste0(input$data_model, ".Rds"))
    TREE <<- loadTree(input$data_model)
    STD_CP_FUN <<- get_complexity_map(TREE)
    CP <<- STD_CP_FUN(input$std_complexity)
    PRUNED_TREE <<- prune(TREE, cp=CP)
    
    if (!is.null(input$liftplot_click)){
      prunedTree <- PRUNED_TREE
      leaf_val_df <- tree_performance_stats(prunedTree)
      SELECTED_NODE <<- get_node_id_by_liftclick(input$liftplot_click, leaf_val_df, input$dataset)
    } else if (!is.null(input$treeplot_click)){
      SELECTED_NODE <<- get_node_id_by_treeclick(PLOTTED_TREE, input$treeplot_click)
    }

  })

  output$treeplot <- renderPlot({
    get_clicks()
    prunedTree <- PRUNED_TREE  # prune(TREE, cp=CP)
    PLOTTED_TREE <<- plot_tree(prunedTree, input$dataset, SELECTED_NODE)
  })

  output$gainplot <- renderPlot({
    get_clicks()
    prunedTree <- PRUNED_TREE
    plot_gain(TREE, prunedTree, dataset=input$dataset, SELECTED_NODE)
  })

  output$liftplot <- renderPlot({
    get_clicks()
    prunedTree <- PRUNED_TREE
    plot_lift(prunedTree, dataset=input$dataset, SELECTED_NODE)
  })
  
  output$node_info <- renderText({
    get_clicks()
    prunedTree <- PRUNED_TREE
    show_node_info(SELECTED_NODE, prunedTree, input$dataset)
  })
  
  get_simplified_rule_table <- function(original_rule){
    if ( length(original_rule) == 0)
      original_rule <- "click on a node in the tree above to see its rule"
    if ( nchar(original_rule) == 0)
      original_rule <- "root node has no rule"
    rule_vector <- strsplit(original_rule, ' & ')[[1]]
    get_rule_key <- function(rule){
      # rule key is feature plus comparison operator
      paste(strsplit(rule, ' ')[[1]][1:2], collapse=' ')
    }
    rule_hash <- new.env(hash = TRUE)
    for (rule in rule_vector){
      key <- get_rule_key(rule)
      rule_hash[[key]] <- rule
    }
    simplified_rules <- unlist(mget(ls(rule_hash), rule_hash))
    simplified_rules <- gsub('%in%', 'in', simplified_rules)
    simplified_rules <- gsub('c(', '(', simplified_rules, fixed=TRUE)
    data.frame(rule=simplified_rules)
  }
  
  output$rule_table <- renderTable({
    get_clicks()
    prunedTree <- PRUNED_TREE
    rule <- unlist(get_node_info(SELECTED_NODE, prunedTree, input$dataset)$rule)
    get_simplified_rule_table(rule)
  })
}

options <- list(width = 1000, height = 1000)

shinyApp(ui, server,  options=options)
```

# Instructions


Choose a model file from the drop-down list. These models are decision trees that have been trained on one subset of data (the training dataset), and their performance has been validated on another subset (the test data). The test set performance helps us understand how well the model works on data it has not seen before.

Use the "complexity adjustment" control to make the model simpler or more complex by pruning the tree.

Display the "training" dataset to see the model performance on the data it was trained on. Differences between training set performance and test set performance usually reflect overfitting.

The lift plot shows a bar for each leaf in the tree. The width of the bar shows how many cases it covers, and the height shows the percent of those cases that are positive. The order of the bars is based on their height in the training set. If bars for the test set are not in order from tallest to shortest, this is a sign of overfitting.

The cumulative gain plot is basically the integral of the lift plot, and it has the same x-axis. The dotted gray line shows what the performance of a perfect classifier would look like, and the yellow curve shows the performance of the unpruned tree. An unpruned tree will usually perform better on the training set than on the test set, due to overfitting.


## TO DO:

* visually distinguish leaf nodes
* cumulative stats for leaf nodes

